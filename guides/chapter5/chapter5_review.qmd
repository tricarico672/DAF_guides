---
title: "Chapter 5 Review"
author: "Anthony Tricarico"
link-citations: true
number-sections: true

mainfont: Times New Roman

format:  
  pdf:
    toc: true                 # Adds a TOC for PDF as well
    margin-top: 1.5cm         # Sets top margin for PDF
    margin-left: 2cm          # Left margin for better readability
    margin-right: 2cm         # Right margin for better readability
    keep-tex: true            # Keeps the intermediate .tex file for debugging/adjustments
    fig-align: center

# Aesthetic options for code rendering
code-block-style: github      # Styles code blocks similar to GitHub's appearance
code-fold: true               # Allows readers to fold/unfold code blocks
code-overflow: wrap           # Ensures code stays within margins
page-layout: article          # Article layout keeps focus on content
editor: visual
---

```{r setup, include = F}
# Global options for R code chunks
knitr::opts_chunk$set(fig.align = "center")
```

# Intro

This is an explanation of the code used for chapter 5. This chapter of the textbook is focused on producing our first forecasts after fitting various models to our time series data. The models in this chapter are meant to be simple so that we can then use them as benchmark methods for more advanced models that will be developed in the next chapters.

Also, this review will assume that you already know and have acquired basic familiarity with the functions explained up until the third chapter. Note that we will not cover all the mathematical aspects included in the book to keep these explanations simple enough, but of course if you feel like it you can just read through the chapter to understand what is really going on behind the scenes. Finally, if you did not cover basic topics in probability theory and hypothesis testing yet, this is the right moment to do so because we will use these concepts frequently in the following sections of this review and those concepts will be useful in the future (e.g., for your quantitative methods class).

# Basic functions for Modeling and Forecasting

This section is an overview of the most important functions that we will use to model our data and produce the first forecasts. First let's import the library that contains the functions we will use.

```{r, results='hide', warning=FALSE}
library(fpp3)
```

## `TSLM()` {#sec-tslm}

This function allows you to fit a linear model using the components of a time series (i.e., trend or seasonality).

```{r}
TSLM(GDP_per_capita ~ trend()) #<1>
```

1.  This is how you specify a formula, on the left of the \~ there is the dependent variable and on its right stands the independent variable (i.e., trend)

In order to use this formula to fit a linear model you need to use the `model()` function.

## `model()`

This is how you use the model function to fit a model to your data:

```{r}
gdppc <- mutate(global_economy, "GDP_per_capita" = GDP / Population) #<1>

(fit <- model(gdppc, trend_model = TSLM(GDP_per_capita ~ trend()))) #<2>
```

1.  create the `gdpcc` table by adding to the `global_economy` dataset a new column specifying the GDP per capita.
2.  we fit the model using the `model()` function and assign its result to the `fit` variable. The result is shown above.

with `model()` as with many other functions you used so far, you just need to specify where the data that are used in the model are contained (`gdppc` in our case) and the name of the column where you want to store the models produced by the formula we described in @sec-tslm . Notice that the name of the column or the formula you want to use for modeling will change based on the model that you want to specify.

### `report()`

This is used to get the results of the model you previously fit to the data.

```{r}
report(filter(fit,Country == 'Sweden')) # see output and evaluate
```

The code above reports the result only for Sweden since we filtered for its model among the many contained in `fit` as you can see from the previous output. Usually `report()` will only work for single models and if we try using the following we get a warning which prompts us to use `filter()` or `select()` to get the output we are looking for:

```{r}
report(fit)
```

## `forecast()`

This is the function that we use to actually make forecasts after fitting a model to our data.

```{r}
forecast(fit, h = "3 years")
```

The arguments in the function is just the name of the model you previously declared and the number of periods you want to use in your forecast. In this case, we are saying that we want forecasts to be produced for three years ahead of the last one. Since, the `global_economy` dataset contains data up until 2017, the forecast will be for the three years after (2018, 2019, and 2020).

We can now filter our forecasts to only get the forecast value for Sweden and plot our forecasts with the following code:

```{r}
fore <- filter(forecast(fit, h = "3 years"), Country == "Sweden") 

autoplot(fore, gdppc) + 
  labs(y = "$US",
       title = "GDP per capita for Sweden") # color='black'
```

Notice that the plot also contains 80% and 95% confidence levels (or intervals) for your forecasts, as represented by the dark blue and light blue ranges, respectively.

# Mean method

The mean method of forecasting simply produces forecasts for future periods that are equal to the mean of the time series considered.

```{r}
recent_prod <- filter_index(aus_production, "1970 Q1" ~ "2004 Q4")
bricks <- dplyr::select(recent_prod, Bricks)
mean_fit <- model(bricks, MEAN(Bricks)) #<1>
```

1.  This is how you specify that you want to use the `MEAN()` method for forecasting. As its argument you pass in the name of the column for which you want to get the forecasts (`Bricks` in this case)

```{r}
tidy(mean_fit)  # extract output (1)
```

Using the `tidy()` function you can have a look at a brief report of the mean_fit model. You can see specifically that the estimate is equal to the mean of the time series considered. Also, you get the p-value which tells you about the significance of the model. Since the p-value is very close to 0, this model is statistically significant and we can use it to produce forecasts.

```{r}
results_list <- mean_fit$'MEAN(Bricks)'[[1]] # extract output (2)
mean_results <- results_list$fit

mean_fc <- forecast(mean_fit, h = 12)
bricks_mean = mutate(bricks,hline=mean_fc$.mean[1]) # add a dashed line
autoplot(mean_fc, bricks, level = NULL) +
  autolayer(bricks_mean,hline,linetype='dashed',color='blue')
```

The lines of code above produce the plot that shows that the forecasts are indeed equal to the mean of the time series.

# Naive method

This method produces forecasts that are just equal to the last observed value in the time series.

```{r}
naive_fit <- model(bricks,NAIVE(Bricks)) #<1>
```

1.  Specify the `NAIVE()` model and fit it to the data

```{r}
naive_fc <- forecast(naive_fit, h = 12) #<1>
```

1.  Produce forecasts using the model specified

```{r}
autoplot(naive_fc, bricks, level = NULL)
```

The plot above just shows how the forecasts produced with this method are equal to the last value observed in the series.

# Seasonal Naive

This method is similar to the Naive method but produces forecasts in the future that are equal to the last seasonal trend observed in the data.

```{r}
snaive_fit <- model(bricks,SNAIVE(Bricks ~ lag("year"))) #<1>
```

1.  specify `SNAIVE()` model indicating that the seasonal trend is observed at a yearly interval

```{r}
snaive_fc <- forecast(snaive_fit, h = 12)
```

```{r}
autoplot(snaive_fc, bricks, level = NULL) #<1>
```

1.  Here the `level` argument is set to `NULL` so that the plot does not contain forecast confidence intervals

```{r}
bricks %>%
  filter_index("Q1 2004"~"Q4 2004") %>%
  autoplot(.vars = Bricks)
```

Notice that the yearly trend looks something like the plot above, which is exactly the shape that the future forecasts follow.

# Drift method

This method interpolates between the first and last observation and the line obtained is then "stretched" into future periods to produce forecasts.

```{r}
drift_fit <- model(bricks, RW(Bricks ~ drift()))
drift_fc <- forecast(drift_fit, h = 12)
autoplot(drift_fc, bricks, level = NULL)
```

To have a basic idea of how this works, we can convince ourselves that the line used in the forecast is actually the line going from the first to the last observation by looking at this plot.

```{r}
T <- length(bricks$Bricks) #getting length of Bricks column
b <- (bricks$Bricks[T] - bricks$Bricks[1])/(T - 1) #equation of a line: slope (row140-row1)/(140-1)
a <- bricks$Bricks[1] 
y <- a + b * seq(1,T,by=1)

DashDR <- tibble(y,Date=bricks$Quarter)
DashDRts <- as_tsibble(DashDR,index=Date)

autoplot(drift_fc, bricks, level = NULL)+
  autolayer(DashDRts,y,color='blue',linetype='dashed')
```

::: {#tip-slope .callout-tip title="Equation of a line"}
Notice that T, b, a are used to compute the interpolated line and that this follows from the equation of a line of the form $y = mx + q$ where $m$ is the slope parameter (b in the code) and $q$ is the intercept (a in the code, which is just the first observation in the time series).
:::

# Train / Test Split

To test how well our model performs we need to test in on data on which it was not trained. This is often done to prevent the problem of *overfitting* which refers to the fact that when a parameters of a model are estimated those perform well on the data that the model has already seen but performs poorly on new data (which is actually what should not happen). To mitigate this problem we divide our dataset into a *train* and a *test* portion.

```{r}
train <- filter_index(aus_production, "1992 Q1" ~ "2006 Q4") #<1>
```

1.  Our train dataset is made only of observations from 1992 to 2006.

```{r}
beer_fit <- model(train, Mean = MEAN(Beer), Naive = NAIVE(Beer),
'Seasonal naive' = SNAIVE(Beer)) #<1>
beer_fc <- forecast(beer_fit, h = 14) #<2>
```

1.  fit three different models using the MEAN, NAIVE, and SNAIVE methods
2.  produce forecasts based on these three models

```{r}
autoplot(beer_fc, train, level = NULL) +
  autolayer(filter_index(aus_production, "2007 Q1" ~ .), .vars = Beer,
  colour = "black") + labs(y = "Megalitres", title = "Forecasts
            for quarterly beer production") +
  guides(colour = guide_legend(title = "Forecast"))
```

Now we can plot how well the three different models perform and we can see that the Seasonal Naive produces more accurate forecasts as it is closer to the original series (black line). Notice, that by the way they were constructed, the models did not see all the data in the series but they were trained only on data up to 2006. Nonetheless, the Seasonal Naive performs pretty well when it tries to make forecasts on data it did not see. In a later section we will see how we can quantify how well a model performs.

## More Examples

```{r}
recent_GOOG <- filter(gafa_stock, Symbol == "GOOG",
                      year(Date) >= 2015)

goog <- mutate(recent_GOOG, day = row_number()) #<1>

google_stock <- update_tsibble(goog, index = day, regular = TRUE)

google_2015 <- filter(google_stock, year(Date) == 2015) # Filter the year of interest

google_fit <- model(google_2015, # Fit the models
Mean = MEAN(Close), Naive = NAIVE(Close),
Drift = NAIVE(Close ~ drift()))
```

1.  the `row_number()` function adds a sequence of numbers representing the row number of each observation (basically a sequence, starts from 1 and ends with the last observation)

By now you should be able to see what's going on in these lines of code. We are filtering, mutating and updating the original tsibble to take as index the day column we just created before. The last three lines is where we fit models to data.

```{r}
google_jan_2016 <- filter(google_stock,
        yearmonth(Date) == yearmonth("2016 Jan")) #<1>

(google_fc <- forecast(google_fit, new_data = google_jan_2016)) #<2>
```

1.  Produce forecasts for the trading days in January 2016
2.  the `new_data` argument is used to specify for which datapoints the forecasts should be produced. In this example, we will be producing forecasts only for the trading days in Jan 2016

```{r}
autoplot(google_fc, google_2015, level = NULL) +
  autolayer(google_jan_2016, Close, colour = "black") + #<1>
  labs(y = "$US", title = "Google daily closing stock prices",
       subtitle = "(Jan 2015 - Jan 2016)") +
  guides(colour = guide_legend(title = "Forecast"))
```

1.  Remember that `autolayer()` just adds another layer to the plot produced with `autoplot()` instead of starting a new plot from a white, empty canvas.

The plot just shows what each model predicts for the Jan 2016 period.

# Residuals

Residuals are the errors that our model makes when being tested. Basically, they represent the difference between the true observed value and what the model predicted that value to be. We now start with an example to understand how residuals fit in our discussion.

```{r}
beer_fit1 <- model(train, SNAIVE(Beer))
(mean_fitted <- augment(beer_fit1)) #<1>
```

1.  fitted values for a single method, along with residuals and innovation residuals

We saw how by using `augment()` on a model we previously fit to our data we can get some more information about how it performs. For instance, we can see that the `.resid` and `.innovation` column contain the residuals and the innovation residuals for each estimate provided by the model. The difference between residuals and innovation residuals is just that innovation residuals are more interpretable when dealing with data that has gone through transformations. However, since that was not the case with our example, we see that the residuals match perfectly with the innovation ones.

```{r, warning=FALSE}
ggplot(mean_fitted, aes(x = Quarter)) +
  geom_line(aes(y = Beer),color='black') +
  geom_line(aes(y = .fitted),color='red') 
autoplot(mean_fitted,.vars = Beer) + # alternative command
  autolayer(mean_fitted,.fitted,color='red')
```

In the plot we see that the predictions produced are very close to the actual series, meaning that the model performs well on the data.

```{r, warning=FALSE}
gg_tsresiduals(beer_fit1)
```

The `gg_tsresiduals()` is a very useful function that allows you to check different assumptions that are imposed on residuals through plots. The usual assumption that are imposed on residuals is that they exhibit:

1.  [homoskedasticity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity) (or exhibit no heteroskedasticity), meaning that there should be no visible pattern in how residuals are distributed over time. This is visible from the plot at the top.
2.  no significant [autocorrelation](https://en.wikipedia.org/wiki/Autocorrelation), this follows from the first assumption as autocorrelation would imply a pattern in the time series that should be exploited but that our model does not capture. You can check this assumption from the ACF plot on the bottom left.
3.  residuals are normally distributed, you can check this from the histogram at the bottom right.

## Assumptions on Residuals (continued)

We continue the discussion here more in-depth.

```{r, warning=FALSE}
aug <- augment(model(google_2015, NAIVE(Close)))
autoplot(aug, .innov) +
  labs(y = "$US", title = "Residuals from the Naive method")
```

In the plot is a graphical representation of the residuals throughout the entire model we estimated using the NAIVE method.

We can also check that the distribution of the residuals closely mirrors a normal distribution with the following lines of code

```{r, warning=FALSE}
p0 <- ggplot(aug, aes(x = .innov)) +
  geom_histogram(aes(y=after_stat(density)), bins = 20, #<1>
  color="black", fill="white")

p0 + stat_function(fun = dnorm, colour = "red", #<2>
  args = list(mean = mean(aug$.innov,na.rm=TRUE),
  sd = sd(aug$.innov,na.rm=TRUE)))
```

1.  The `after_stat()` function used as a y `aesthetic` inside the `geom_histogram()` layer of the ggplot, is used to specify that we want the `density` to be plotted on the y axis as opposed to the default behavior of the function which is to plot the `count`.
2.  The `stat_function()` is used to add a layer which plots a probability distribution specified by the argument `fun`. In this case, we are plotting the density of the normal distribution (`dnorm()`) in red. Since it is another layer, it is superimposed to the histogram. Since each normal distribution is uniquely identified by a `mean` and a `standard deviation`, we need to pass these values inside the `dnorm()` function using the `args` argument as a `list`.

```{r}
autoplot(ACF(aug, .innov)) +
  labs(title = "Residuals from the Naive method")
```

By computing the ACF we can also plot it and check for the presence of significant autocorrelation of residuals at different time lags.

## Portmanteau Tests

These kinds of test are used when we want to assess the level of autocorrelation in a time series across a number $k$ of lags, where $k \in \mathbb{N}$ (i.e., notation for saying that $k$ is a [natural number](https://www.cuemath.com/numbers/natural-numbers/)). There are two main tests that your book discusses.

### Box-Pierce Test

```{r}
features(aug, .innov, box_pierce, lag = 10)
```

With this we compute the Box-Pierce statistic on the data and obtain its related p-value. Generally a p-value greater than 0.1 will be a good way of assessing that the residuals are randomly distributed and that there is no significant pattern arising from their actual distribution.

### Ljung-Box Test

```{r}
features(aug, .innov, ljung_box, lag = 10)
```

Another approach is using the Ljung-Box test. It has been shown that the two tests yield comparable results, however, this test has been shown to be more precise overall so it is recommended that you use this one from now on. Remember that to use the `features()` function you need to specify the augmented model fit (`aug` in the example), select the `.innov` column containing the innovation residuals, and then specify that you want to compute the `ljung_box` statistic with a `lag` of 10 (due to the data being non-seasonal in this case).

::: callout-important
The book suggests using ℓ=10 (i.e., `lag = 10`) for non-seasonal data and ℓ=2m for seasonal data, where m is the period of seasonality. However, the test is not good when ℓ is large, so if these values are larger than T/5, then use ℓ=T/5. T in this case refers to the total number of observations that you are using to carry out the test (i.e., number of rows in your dataset).
:::

#### More examples

We can now use this method to check if the residuals produced by other models present patterns or are auto-correlated.

```{r}
fit <- model(google_2015, RW(Close ~ drift()))
tidy(fit) #<1>
```

1.  remember that the estimate obtained using the drift method is $\frac{(y_{T}-y_{1})}{(T-1)}$ = (759-522)/251 = 0.9439931. We saw this also in @tip-slope

```{r}
features(augment(fit), .innov, ljung_box, lag=10)
```

then we compute the Ljung Box statistic and see that the p-value is \> .1 and therefore we can assume that residuals are randomly distributed (no pattern)

```{r}
hilo(forecast(model(google_2015, NAIVE(Close)), h = 10))
```

Finally we use the `hilo()` function to extract confidence (prediction) intervals for each prediction obtained from a NAIVE model.

## Bootstrapped residuals

We can use our model to generate more observations. These observations would be generated based on what has been observed in previous periods, but they would still vary. In this case, we are building a simulation to test our model on multiple different future possibilities. The method that is used to obtain new random observations starting from a sample is called **bootstrap**.

To generate these bootstrapped residuals we can use the `generate()` function in R. Alternatively, we could also just set the `bootstrap` argument in the `forecast()` to `TRUE` and specify how many `times` we want to run the bootstrap simulation.

```{r}
(fore <- forecast(model(google_2015, NAIVE(Close)), h = 10,
        bootstrap = TRUE, times = 1000)) #<1>
```

1.  This specifies that the bootstrapping process will perform 1000 simulations. Each simulation generates a possible future path for the time series. These simulations are then aggregated to produce the final forecast distribution.

Finally we take a look at the final forecast distribution along with the prediction intervals

```{r}
hilo(fore)
```

# Final lines of code

## More examples on model fitting and diagnosis of residuals

```{r}
us_retail_employment <- filter(us_employment, year(Month) >= 1990,
                        Title == "Retail Trade") #<1>

US_model_0 <- model(us_retail_employment,
              STL(Employed ~ trend(window = 7), robust = TRUE)) #<2>
```

1.  filter the dataset
2.  specify the model formula using an STL-based decomposition using only the trend component

```{r}
US_model_1 <- select(components(US_model_0), -.model) #<1>

(US_fore <- forecast(model(US_model_1, NAIVE(season_adjust)))) #<2>
```

1.  get rid of the `.model` column
2.  produce forecasts using the NAIVE method on the seasonally adjusted series

Finally, we plot the results from our model which is based on seasonally adjusted data.

```{r}
autoplot(US_fore, US_model_1) +
  labs(y = "Number of people", title = "US retail employment")
```

Another similar example follows below.

```{r}
fit_dcmp <- model(us_retail_employment,
    stlf = decomposition_model(STL(Employed ~ trend(window = 7),
          robust = TRUE), NAIVE(season_adjust)))
```

We start from the decomposing the model using an STL decomposition as before and then we fit a NAIVE model to the seasonally adjusted time series. We then plot the results.

```{r}
autoplot(forecast(fit_dcmp), us_retail_employment) +
  labs(y = "Number of people", title = "Monthly US retail employment")
```

We can also plot the residuals of the model by using:

```{r, warning=F}
gg_tsresiduals(fit_dcmp)
```

From here we notice some patterns in how those are distributed and the autocorrelation at different time lags. We now want to compute the mean of the innovation residuals.

```{r}
features(augment(fit_dcmp), .innov, list(avg = ~ mean(.,na.rm=TRUE)))
```

A simpler way to get the mean of the residuals **when you only estimate one model** is shown below.

```{r}
mean(augment(fit_dcmp)$.innov, na.rm=TRUE)
```

## Subsetting

Subsetting refers to extracting only some rows in a dataset. Subsetting is usually a synonym of filtering the data and this is also shown in code. Here is an example:

```{r}
filter(aus_production, year(Quarter) >= 1995) #<1>
```

1.  only get the rows where the year of the observation is greater than or equal to 1995

In case you do not have a specific filtering condition to use, you can use slice to get a subset of your data.

```{r}
slice(aus_production, n()-19:0) #<1>
```

1.  last 20 observations obtained using the `n()` function which returns the total number of rows (n) in the aus_production dataset. Then the slice only selects the rows from n-19 to the end.

```{r}
slice(group_by(aus_retail, State, Industry), 1:12) # working with groups
```

## Forecast errors

```{r}
recent_production <- filter(aus_production, year(Quarter) >= 1992) #<1>

beer_train <- filter(recent_production, year(Quarter) <= 2007) #<2>
```

1.  get rows where year is after 1992 (included)
2.  train / test split

```{r}
beer_fit <- model(beer_train, Mean = MEAN(Beer),
    Naive = NAIVE(Beer),
    'Seasonal naive' = SNAIVE(Beer),
    Drift = RW(Beer ~ drift()))
```

Fit three models to our training dataset. Finally, produce forecasts using our model.

```{r}
(beer_fc <- forecast(beer_fit, h = 10))
```

And plot the forecasts without plotting the prediction intervals (i.e., `level = NULL`).

```{r}
autoplot(beer_fc, recent_production, level = NULL) +
  labs(y = "Megalitres", title = "Forecasts for quarterly beer production") +
  guides(colour = guide_legend(title = "Forecast"))
```

Finally, we evaluate the performance of our model by producing an accuracy table.

```{r}
accTable <- accuracy(beer_fc, recent_production)
select(accTable,.model,RMSE,MAE,MAPE)
```

This allows us to get the main accuracy measures for the different models fit to the data. From the table we see that the Seasonal naive model scores the lowest on the error metrics, and therefore is the best model among those proposed.
